# Building a Micro Neural Network from Scratch

Welcome to the playground of neural networks! 🧠✨ This repository is a delightful journey through constructing a mini neural network from scratch, with inspiration from Andrej Karpathy’s amazing tutorials. We dive deep into the mechanics of neural networks, unraveling the mysteries of layers, MLPs, and the inner workings of PyTorch and Numpy. Buckle up for a ride through math, code, and a dash of magic!

## Table of Contents

- [Overview](#overview)
- [Usage](#usage)
- [Training the MLP](#training-the-mlp)
- [Credits](#credits)

## Overview

Ever wondered how those fancy neural networks work under the hood? Here, we build our own from scratch, demystifying:
- **Neurons**: The tiny brain cells of our network, doing all the math and thinking.
- **Layers**: Groups of neurons working together to form a more complex brain.
- **Multi-Layer Perceptron (MLP)**: A stack of layers that transforms our input into something meaningful.
- **Backpropagation**: The magic behind learning, where the network tweaks itself to get better and better.

## Usage
**Neurons**:
Imagine a neuron as a tiny calculator that takes some numbers, multiplies them by its own secret numbers (weights), adds a little extra (bias), and then decides what to output. It’s like a secret agent doing math tricks! 🕵️‍♂️

**Layers**:
Now, take a bunch of these secret agent neurons and put them in a room together — that’s a layer! Each neuron does its trick and passes the result on. Together, they can handle more complex tasks than a single neuron. 🤹‍♀️

**Multi-Layer Perceptron (MLP)**:
Stack several layers together, and you get a Multi-Layer Perceptron (MLP). Think of it as a brain with multiple regions, each adding more sophistication to the input. It starts with raw data and ends up giving you a polished output. 🎨🧩

## Training the MLP
Training is where the real fun begins! The MLP makes guesses and then checks how wrong they are. Using a bit of math magic (backpropagation), it tweaks all those weights and biases to get better over time. It’s like teaching a puppy to fetch — trial, error, and lots of learning. 🐶🎓

Create an MLP: Define an MLP with a specific structure.
Feed it data: Give it some numbers to chew on.
See the output: Watch it transform your input into something cool.
Train it: Let it learn from its mistakes and improve.

## Credits
A big shout-out to Andrej Karpathy for his educational videos that inspired this project! 📚👏

