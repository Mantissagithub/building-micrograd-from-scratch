# Building a Micro Neural Network from Scratch

Welcome to the playground of neural networks! ğŸ§ âœ¨ This repository is a delightful journey through constructing a mini neural network from scratch, with inspiration from Andrej Karpathyâ€™s amazing tutorials. We dive deep into the mechanics of neural networks, unraveling the mysteries of layers, MLPs, and the inner workings of PyTorch and Numpy. Buckle up for a ride through math, code, and a dash of magic!

## Table of Contents

- [Overview](#overview)
- [Usage](#usage)
- [Training the MLP](#training-the-mlp)
- [Credits](#credits)

## Overview

Ever wondered how those fancy neural networks work under the hood? Here, we build our own from scratch, demystifying:
- **Neurons**: The tiny brain cells of our network, doing all the math and thinking.
- **Layers**: Groups of neurons working together to form a more complex brain.
- **Multi-Layer Perceptron (MLP)**: A stack of layers that transforms our input into something meaningful.
- **Backpropagation**: The magic behind learning, where the network tweaks itself to get better and better.

## Usage
**Neurons**:
Imagine a neuron as a tiny calculator that takes some numbers, multiplies them by its own secret numbers (weights), adds a little extra (bias), and then decides what to output. Itâ€™s like a secret agent doing math tricks! ğŸ•µï¸â€â™‚ï¸

**Layers**:
Now, take a bunch of these secret agent neurons and put them in a room together â€” thatâ€™s a layer! Each neuron does its trick and passes the result on. Together, they can handle more complex tasks than a single neuron. ğŸ¤¹â€â™€ï¸

**Multi-Layer Perceptron (MLP)**:
Stack several layers together, and you get a Multi-Layer Perceptron (MLP). Think of it as a brain with multiple regions, each adding more sophistication to the input. It starts with raw data and ends up giving you a polished output. ğŸ¨ğŸ§©

## Training the MLP
Training is where the real fun begins! The MLP makes guesses and then checks how wrong they are. Using a bit of math magic (backpropagation), it tweaks all those weights and biases to get better over time. Itâ€™s like teaching a puppy to fetch â€” trial, error, and lots of learning. ğŸ¶ğŸ“

Create an MLP: Define an MLP with a specific structure.
Feed it data: Give it some numbers to chew on.
See the output: Watch it transform your input into something cool.
Train it: Let it learn from its mistakes and improve.

## Credits
A big shout-out to Andrej Karpathy for his educational videos that inspired this project! ğŸ“šğŸ‘

